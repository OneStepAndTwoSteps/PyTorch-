{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=10, bias=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(in_features=10,out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、张量的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.2,3.4]).dtype ## 获取张量的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor) ## torch.set_default_tensor_type 用于设置默认的数据类型（该函数只支持浮点型的设置）\n",
    "torch.tensor([1.2,3.4]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 浮点型转为其他数据类型的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.dtype： torch.float64\n",
      "a.long： torch.int64\n",
      "a.int： torch.int32\n",
      "a.float： torch.float32\n"
     ]
    }
   ],
   "source": [
    "a= torch.tensor([1.2,3.4])\n",
    "print(\"a.dtype：\",a.dtype)\n",
    "print(\"a.long：\",a.long().dtype)  \n",
    "print(\"a.int：\",a.int().dtype)    \n",
    "print(\"a.float：\",a.float().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置回 tensor 默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor) ## torch.set_default_tensor_type 用于设置默认的数据类型（该函数只支持浮点型的设置）\n",
    "torch.tensor([1.2,3.4]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取默认的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、张量的生成\n",
    "\n",
    "\n",
    "tensor 和 Tensor 的区别：\n",
    "\n",
    "#### 区别1：\n",
    "\n",
    "* torch.Tensor(data)：是将输入的data转化torch.FloatTensor\n",
    "* torch.tensor(data)：(当你未指定dype的类型时)将data转化为torch.FloatTensor、torch.LongTensor、torch.DoubleTensor等类型，转化类型依据于data的类型或者dtype的值\n",
    "\n",
    "#### 区别2：\n",
    "\n",
    "* 使用如下语句：tensor_without_data = torch.Tensor()可以创建一个空的FloatTensor，而当你使用tensor_without_data = torch.tensor()时候则会报错。\n",
    "\n",
    "#### 总结：\n",
    "\n",
    "* 所以 torch.Tensor 应该说是同时具有 torch.tensor 和 torch.empty 的功能，但是使用 torch.Tensor 可能会令人困惑，所以最好还是使用 torch.tensor 和 torch.empty ，而不是 torch.Tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、 torch.tensor()  函数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A= torch.tensor([[1,1],[2,2]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用shape 和size 可以查看张量的维度和形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)   ## 获取张量的维度\n",
    "print(A.size())  ## 获取张量的形状大小\n",
    "\n",
    "## size 和 shape 的区别：1、shape 是属性 ，size 是方法  2、size 可以传参查看某个子张量矩阵的维度大小\n",
    "\n",
    "print(A.numel())   ## 计算张量中包含的元素的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 torch.tensor 时可以使用 dtype 来指定张量的数据类型，使用 requires_grad 来指定是否需要计算梯度，只有计算了梯度的张鲁昂才能在深度网络优化时根据梯度大小进行更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([1,2,3],dtype=torch.float32,requires_grad=True)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面计算 sum($B^2$) 的梯度\n",
    "\n",
    "需要注意的是只有浮点型才能计算张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "y= B.pow(2).sum()\n",
    "print(y.backward())\n",
    "print(B.grad)       # B方 的梯度为 2B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、 torch.Tensor()  函数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.Tensor([1,2,3,4])    ## 根据 python 列表生成张量c\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.8750, 0.0000],\n",
       "        [2.0000, 0.0000, 2.1250]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = torch.Tensor(2,3)             ## 创建具有特定大小的张量\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.5760, 0.5961, 0.5157],\n",
      "        [0.9368, 0.6834, 0.9161]])\n"
     ]
    }
   ],
   "source": [
    "## 使用torch.**_like() 系列函数生成指定张量维度相同、性质相似的张量\n",
    "print(torch.ones_like(D))   ## 创建一个与D相同大小和类型的全1张量\n",
    "print(torch.zeros_like(D))  ## 创建一个与D相同大小和类型的全0张量\n",
    "print(torch.rand_like(D))  ## 创建一个与D相同大小和类型的随机张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.dtype： torch.float32\n",
      "E.dtype： torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 创建一个类型相反但是尺寸不同的张量\n",
    "E = [[1,2],[3,4]]\n",
    "E = D.new_tensor(E)\n",
    "print('D.dtype：',D.dtype)\n",
    "print('E.dtype：',E.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、张量和NumPy数据相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "F = np.ones((3,3))\n",
    "## 使用 torch.as_tensor() 函数和 torch.from_numpy() 函数可以将数组转为 PyTorch 张量。\n",
    "Ftensor = torch.as_tensor(F)\n",
    "print(Ftensor)\n",
    "\n",
    "Ftensor = torch.from_numpy(F)\n",
    "Ftensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、随机数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1115)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 通过指定指定均值和标准差生成随机数\n",
    "torch.manual_seed(123)                        ## 指定随机种子，不指定每一次运行的结构都不一样\n",
    "A=torch.normal(mean=0.0,std=torch.tensor(1.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0283,  2.8440, -1.1658, -3.5613])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 通过指定指定均值和标准差生成随机数\n",
    "torch.manual_seed(123)\n",
    "A=torch.normal(mean=0.0,std=torch.arange(1,5.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、其他生成张量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start=0,end=10,step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start=0,end=10,steps=5) ## 在范围内生成固定数量的等间隔张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e+02, 1.0000e+05, 3.1623e+07, 1.0000e+10])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(start=0,end=10,steps=5) ## 生成以对数为间隔的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、张量操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、改变张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A= torch.arange(12.0).reshape(3,4) ## reshape 设置张量的大小\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(input=A,shape=(2,-1))   ## 设置形状 （行为2，列为任意）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = A.resize_(2,6)  ## 设置形状\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.arange(10.0,19.0).reshape(3,3)  \n",
    "A.resize_as_(B)  ##  可以让 A 设置成 B 的形状\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10., 11.]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12.0).reshape(2,6)\n",
    "B = torch.unsqueeze(A,dim=0)    ## 在指定维度插入尺寸为1的新张量\n",
    "print(B.shape)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape： torch.Size([1, 2, 6, 1])\n",
      "D.shape： torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "## torch.squeeze()   ## 函数移除所有维度为1 的维度\n",
    "C = B.unsqueeze(dim=3)\n",
    "print('C.shape：',C.shape)\n",
    "D = torch.squeeze(C)\n",
    "print('D.shape：',D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用 .enpand 方法扩展张量\n",
    "A=torch.arange(3)\n",
    "B=A.expand(3,-1)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用 .enpand_as() 方法扩展张量\n",
    "\n",
    "C = torch.arange(6).reshape(2,3)\n",
    "B = A.expand_as(C)               ## 张量A根据C的形状来进行扩展，得到新的张量\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 6])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用repeat 方法扩展张量，根据指定形状重复填充\n",
    "D = B.repeat(1,2,2)\n",
    "print(D)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、获取张量中的元素\n",
    "\n",
    "#### 在张量中利用切片提取元素和在Numpy 中使用的方法是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "## 利用切片提取元素\n",
    "A= torch.arange(12).reshape(1,3,4)\n",
    "print(A)\n",
    "print(A[0])\n",
    "print(A[0,0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch 中还可以将索引设置为相应的布尔值，然后提取为真是条件下的内容，例如找到A中取值大于5的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, -1, -2, -3],\n",
      "         [-4, -5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = -A\n",
    "print(torch.where(A>5,A,B))   ## 当满足 A >5时返回x所对应位置值，为false时返回y的值\n",
    "\n",
    "## 获取 A 中大于5 的元素\n",
    "A[A>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、拼接和拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape： torch.Size([2, 3])\n",
      "B.shape： torch.Size([2, 3])\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n",
      "\n",
      "\n",
      "C.shape： torch.Size([4, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n",
      "\n",
      "\n",
      "D.shape： torch.Size([2, 6])\n",
      "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n",
      "        [ 3.,  4.,  5.,  6.,  8., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  1.,  2.,  0.,  2.,  4.],\n",
       "        [ 4.,  3.,  4.,  5.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在给定维度中链接给定的张量序列\n",
    "A = torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(0,10,6).reshape(2,3)\n",
    "\n",
    "\n",
    "print('A.shape：',A.shape)\n",
    "print('B.shape：',B.shape)\n",
    "print(A)\n",
    "print(B)\n",
    "print('\\n')\n",
    "\n",
    "## 在0维度连接张量\n",
    "C = torch.cat((A,B),dim=0)\n",
    "print('C.shape：',C.shape)\n",
    "print(C)\n",
    "print('\\n')\n",
    "## 在1维度连接张量\n",
    "D = torch.cat((A,B),dim=1)\n",
    "print('D.shape：',D.shape)\n",
    "print(D)\n",
    "\n",
    "E =torch.cat((A[:,1:2],A,B),dim=1)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 0.,  2.,  4.],\n",
      "         [ 6.,  8., 10.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "\n",
      "\n",
      "tensor([[[ 0.,  0.],\n",
      "         [ 1.,  2.],\n",
      "         [ 2.,  4.]],\n",
      "\n",
      "        [[ 3.,  6.],\n",
      "         [ 4.,  8.],\n",
      "         [ 5., 10.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "## torch.stack 可以将多个张量按照指定的新维度进行拼接\n",
    "F = torch.stack((A,B),dim=0)\n",
    "print(F)\n",
    "print(F.shape)  ## 新创建了一个维度\n",
    "\n",
    "print('\\n')\n",
    "G = torch.stack((A,B),dim=2) \n",
    "print(G)\n",
    "print(G.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  1.,  2.,  0.,  2.,  4.],\n",
      "        [ 4.,  3.,  4.,  5.,  6.,  8., 10.]])\n",
      "\n",
      "\n",
      "(tensor([[1., 0., 1., 2., 0., 2., 4.]]), tensor([[ 4.,  3.,  4.,  5.,  6.,  8., 10.]]))\n",
      "\n",
      "\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n",
      "\n",
      "\n",
      "tensor([[1., 0., 1.],\n",
      "        [4., 3., 4.]])\n",
      "tensor([[2., 0., 2.],\n",
      "        [5., 6., 8.]])\n",
      "tensor([[ 4.],\n",
      "        [10.]])\n",
      "\n",
      "\n",
      "tensor([[0.],\n",
      "        [3.]])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "## torch.chunk 可以将张量分割为特定数量的块；torch.split  可以将张量分割为特定数量块的同时指定每个块的大小\n",
    "print(E)\n",
    "print('\\n')\n",
    "\n",
    "print(torch.chunk(E,2,dim=0))\n",
    "print('\\n')\n",
    "\n",
    "D1,D2 = torch.chunk(D,2,dim=1) \n",
    "print(D1)\n",
    "print(D2)\n",
    "print('\\n')\n",
    "\n",
    "## 特别的如果给定维度dim 的张量大小不能被块整除，则最后一块将最小。\n",
    "E1,E2,E3 = torch.chunk(E,3,dim=1) \n",
    "print(E1)\n",
    "print(E2)\n",
    "print(E3)\n",
    "print('\\n')\n",
    "\n",
    "## 将张量切分为块，并且指定每个块的大小\n",
    "D1,D2,D3 = torch.split(D,[1,2,3],dim=1)\n",
    "print(D1)\n",
    "print(D2)\n",
    "print(D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、张量的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  tensor([1, 2, 3, 4, 5, 6])\n",
      "B:  tensor([1, 2, 3, 4, 5, 6])\n",
      "C:  tensor([[1, 2, 3, 4, 5, 6]])\n",
      "\n",
      "\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "\n",
      "\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([1,2,3,4,5,6])\n",
    "B = torch.arange(1,7)\n",
    "C = torch.unsqueeze(B,dim=0)\n",
    "\n",
    "print('A: ',A)\n",
    "print('B: ',B)\n",
    "print('C: ',C)\n",
    "print('\\n')\n",
    "\n",
    "## 计算元素是否相等 torch.eq() 函数\n",
    "print(torch.eq(A,B))\n",
    "print(torch.eq(A,C))\n",
    "print('\\n')\n",
    "\n",
    "## 判断两个张量是否具有相同的形状和元素 torch.equal() 函数\n",
    "print(torch.equal(A,B))\n",
    "print(torch.equal(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "\n",
      "\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n",
      "\n",
      "\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "\n",
      "\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n",
      "\n",
      "\n",
      "tensor([False, False, False, False, False, False])\n",
      "\n",
      "\n",
      "tensor([False, False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "## 逐元素比较大于等于\n",
    "print(torch.ge(A,B))\n",
    "print(torch.ge(A,C))\n",
    "print('\\n')\n",
    "\n",
    "## 逐元素比较大于\n",
    "print(torch.gt(A,B))\n",
    "print(torch.gt(A,C))\n",
    "print('\\n')\n",
    "\n",
    "## 逐元素比较小于等于\n",
    "print(torch.le(A,B))\n",
    "print(torch.le(A,C))\n",
    "print('\\n')\n",
    "\n",
    "## 逐元素比较小于\n",
    "print(torch.lt(A,B))\n",
    "print(torch.lt(A,C))\n",
    "print('\\n')\n",
    "\n",
    "## 逐元素比较不相等\n",
    "print(torch.ne(A,B))\n",
    "print('\\n')\n",
    "\n",
    "## 判断缺失值\n",
    "print(torch.isnan(torch.tensor([1,2,3,float('nan')])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、基本运算\n",
    "\n",
    "张量的基本运算是逐元素运算，很简单，以乘法为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[10.0000, 11.1111, 12.2222, 13.3333, 14.4444],\n",
      "        [15.5556, 16.6667, 17.7778, 18.8889, 20.0000]])\n",
      "tensor([[  0.0000,  11.1111,  24.4444,  40.0000,  57.7778],\n",
      "        [ 77.7778, 100.0000, 124.4445, 151.1111, 180.0000]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(10).reshape(2,5)\n",
    "B = torch.linspace(10,20,steps=10).reshape(2,5)\n",
    "print(A)\n",
    "print(B)\n",
    "print(A*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的矩阵运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 30,  80],\n",
       "        [ 80, 255]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 矩阵的转置\n",
    "C = torch.t(A)\n",
    "print(C)\n",
    "\n",
    "## 矩阵乘法,注意，矩阵 A 的列数要等于 C 的行数\n",
    "A.matmul(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计相关的运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,\n",
      "          0.3239, -0.1085]])\n",
      "tensor(0.3239)\n",
      "tensor(8)\n",
      "\n",
      "\n",
      "tensor(-1.1969)\n",
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "A=torch.randn([1,10])  ## 生成随机张量\n",
    "print(A)\n",
    "print(A.max())       ## torch.max() 查看最大值\n",
    "print(A.argmax())    ## torch.argmax() 查看最大值的位置\n",
    "print('\\n')\n",
    "print(A.min())       ## torch.max() 查看最大值\n",
    "print(A.argmin())    ## torch.argmax() 查看最大值的位置\n",
    "\n",
    "B =A.reshape(2,5)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(values=tensor([[-1.1969, -0.9724, -0.7550, -0.3696, -0.2404, -0.1115, -0.1085,  0.1204,\n",
       "          0.2093,  0.3239]]), indices=tensor([[4, 6, 7, 2, 3, 0, 9, 1, 5, 8]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(values=tensor([[ 0.3239,  0.2093,  0.1204, -0.1085, -0.1115, -0.2404, -0.3696, -0.7550,\n",
       "         -0.9724, -1.1969]]), indices=tensor([[8, 5, 1, 9, 0, 3, 2, 7, 6, 4]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bsort:  tensor([[-1.1969, -0.3696, -0.2404, -0.1115,  0.1204],\n",
      "        [-0.9724, -0.7550, -0.1085,  0.2093,  0.3239]])\n",
      "Bsort_id:  tensor([[4, 2, 3, 0, 1],\n",
      "        [1, 2, 4, 0, 3]])\n"
     ]
    }
   ],
   "source": [
    "## torch.sort() 可以对张量进行排序，输出从小到大的排序结果和对应的元素索引位置\n",
    "### 1D 张量\n",
    "display(torch.sort(A))\n",
    "\n",
    "display(torch.sort(A,descending=True))\n",
    "print('\\n')\n",
    "\n",
    "### 2D张量\n",
    "Bsort,Bsort_id=torch.sort(B)\n",
    "print('Bsort: ',Bsort)\n",
    "print('Bsort_id: ',Bsort_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(values=tensor([[0.3239, 0.2093]]), indices=tensor([[8, 5]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(values=tensor([[ 0.2093,  0.1204, -0.3696,  0.3239, -0.1085],\n",
       "        [-0.1115, -0.9724, -0.7550, -0.2404, -1.1969]]), indices=tensor([[1, 0, 0, 1, 1],\n",
       "        [0, 1, 1, 0, 0]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(values=tensor([-0.9724]), indices=tensor([6]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 获取张量中最大的前K个数值和索引位置\n",
    "display(torch.topk(A,2))\n",
    "\n",
    "### 2D 数据\n",
    "display(torch.topk(B,2,dim=0))\n",
    "\n",
    "## 获取张量中最小的第K个数值和索引位置\n",
    "display(torch.kthvalue(A,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Tensor B -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- mean -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3596],\n",
       "        [-0.2606]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- sum -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7981],\n",
       "        [-1.3028]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- cumsum -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.0089, -0.3607, -0.6012, -1.7981],\n",
       "        [ 0.2093, -0.7631, -1.5181, -1.1942, -1.3028]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- median -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.median(values=tensor([-0.2404, -0.1085]), indices=tensor([3, 4]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- cumprod -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115, -0.0134,  0.0050, -0.0012,  0.0014],\n",
       "        [ 0.2093, -0.2035,  0.1536,  0.0498, -0.0054]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- std -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5018, 0.5781])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('-------- Tensor B -------')\n",
    "display(B)\n",
    "\n",
    "## torch.mean() 平均数\n",
    "print('-------- mean -------')\n",
    "display(torch.mean(B,dim=1,keepdim=True))          ## keepdim 用于指定是否保留原维度，为False 时mean 会减少一维\n",
    "# display(torch.mean(B,dim=1).shape)               ## torch.Size([2])\n",
    "# display(torch.mean(B,dim=1,keepdim=True).shape)  ## torch.Size([2, 1])\n",
    "\n",
    "## torch.sum()  加法\n",
    "print('-------- sum -------')\n",
    "display(torch.sum(B,dim=1,keepdim=True))          \n",
    "\n",
    "\n",
    "## torch.cumsum()  累加\n",
    "print('-------- cumsum -------')\n",
    "display(torch.cumsum(B,dim=1))          \n",
    "\n",
    "\n",
    "## torch.median()\n",
    "print('-------- median -------')\n",
    "display(torch.median(B,dim=1))          \n",
    " \n",
    "## torch.cumprod() 累乘\n",
    "print('-------- cumprod -------')\n",
    "display(torch.cumprod(B,dim=1))          \n",
    "\n",
    "\n",
    "## torch.std() 标准差\n",
    "print('-------- std -------')\n",
    "display(torch.std(B,dim=1))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
